---
title: "R Notebook"
output: html_notebook
---

```{r}
# install.packages('reshape','psych','factoextra')
library(factoextra)
library(psych)
library(reshape)
library(ggplot2)
library(dplyr)
library(tidyr)
library(reshape)
library(glmnet)
```

```{r}
df <- read.csv('../data/restaurants_train.csv')
head(df,5)
```

```{r}
str(df)
```

```{r}
print('Count of missing NAs')
sapply(df, function(x) sum(is.na(x)))
```

## Using PCA on high-dimensional data

Given the data set we have, we have a large number of features and we want to apply an unsupervised method to determine insights. PCA simplifies the features into fewer components to help us visualize hidden patterns in our data.

Identifying if the data is spherical?

```{r}
corrMatrix <- cor(df[,-c(1,2)])
psych::cortest.bartlett(corrMatrix)
```

Since p-value \< 0.05, we reject the H0 that the data is spherical.

```{r}
# Applying PCA
df_scaled <- scale(df)
# df_scaled <- df
df_scaled <- df_scaled[,!(colnames(df_scaled) %in% c("income","star_ratings"))]
df_pca <- prcomp(df_scaled)
summary(df_pca)
```

As observed, we have a large number of PCA components that have been generated from the data. Our next step is to determine what PCA component's explains most of the variance of our data.

```{r}
# PCA Variance ratio
pca_summary <- summary(df_pca)
cumulative_varprop <- pca_summary$importance["Cumulative Proportion",]
# Visual
barplot(cumulative_varprop)
abline(h=0.80, col = 'red', lty = 2)
title(main = "PCA explained variance ratio",
      xlab = "PCA components", ylab = "Variance Ratio")
```

As observed, we generated 32 PCA components and 85% of the variance by PCA1 - PCA8. Since a large portion of our variance is observed in these components, we can now reduce the components into 8 and perform some analysis. It will be hard to include all the components since some of the them have low explained variance specially on the right most PCA's which may not be that significant.

```{r}
# PCA Loadings
pca_loadings <- data.frame(features=row.names(df_pca$rotation),df_pca$rotation) 

pca_reduced <- 
  melt(pca_loadings,id=c("features")) %>%
    filter(variable %in% paste0('PC',1:8)) %>%
    group_by(variable) %>%
    top_n(5,abs(value)) %>% 
    ungroup() %>%
    mutate(sign = ifelse(value>0, "+", "-"))


pca_reduced[pca_reduced$variable %in% c("PC1","PC2","PC3","PC4"),] %>%
  ggplot(aes(x=abs(value),y=features,fill=sign)) + 
  geom_bar(stat='identity') +
  facet_wrap(~variable, scales='free_y') +
  theme_classic() +
  labs(x=NULL)

```

```{r}
pca_reduced[pca_reduced$variable %in% c("PC5","PC6","PC7","PC8"),] %>%
  ggplot(aes(x=abs(value),y=features,fill=sign)) + 
  geom_bar(stat='identity') +
  facet_wrap(~variable, scales='free_y') +
  theme_classic() +
  labs(x=NULL)
```

We limit the loadings of each principal components to their top 5 loading values (abs(value)) for interpretability of principal components.

**Results:**

-   PCA1 - In contrast of *low* values of social media engagement and advertising to *high* unemployment rate, economic rate, and average income.

-   PCA2 - *low* values of features related to holistic employment growth and competitor factors.

-   PCA3 - *high* values of features related to employee growth and weather conditions.

-   PCA4 - *high* values of features related to competitor factors and weather conditions.

-   PCA5 - In contrast of *high* values for inventory turnover, staff turn over and order processing time to *low* values of restaurant's value for money and promos.

-   PCA6 - *high* values for variety of products (ie. vegetarian) and accessibility.

-   PCA7 - In contrast of *low* values for unemployment & economic growth rate, and average income to variety of products including seasonality.

-   PCA8 - In contrast of *low* values of variety of products together with seasonal promos to *high* employee satisfactory performance/growth.

## Association with income/rating

Now we interpret each PC's, we can now associate it to income or rating by using Linear Regression.

```{r}
# Filtering to PC1-8
reduced_col <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8")
pca_components <- df_pca[['x']][, colnames(df_pca[['x']]) %in% reduced_col]

df_pca_lm <- data.frame(df[,c("income","star_ratings")],pca_components)
```

```{r}
# Linear Reg - PCs (Income)
pca_reg <- lm(data=df_pca_lm[,-2], formula = income ~ .)
summary(pca_reg)
```

Interpretation:

-   PC3 having the highest estimate (+ coefficient) means that this principal component which relates to employee growth and weather conditions has the most increasing effect on average on the income of the restaurant. PC6 which relates to restaurant's variety of products and accessibility gave us also a large increasing effect on income next to PC3. Basically, these two PC's estimates contributes large in the increase of income on average.

-   PC2, PC4, and PC7 having a large contribution on lowest estimate impacts the decreasing income of restaurant on average. PC2 and PC4 relates to poor quality of employee growth/competitor factors and extreme weather events/competitor factors. On the other hand, PC7 relates to combination of low unemployment/economic growth rate and good variety of products of the restaurant.
